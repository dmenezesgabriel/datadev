{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207e423a",
   "metadata": {},
   "source": [
    "# Glue Spark\n",
    "\n",
    "- RDD: Datasets which storage data in distributed format\n",
    "- Transformations: Datasets iterations\n",
    "- Actions: Run the transformations on RDD\n",
    "- Output data: Result of transformations\n",
    "\n",
    "Spark runs lazy, so transformations are not immediately executed, they stay in a logical plan to be executed when the actions are called.\n",
    "\n",
    "Actions examples:\n",
    "\n",
    "- `take()`\n",
    "- `collect()`\n",
    "- `show()`\n",
    "- `save()`\n",
    "- `count()`\n",
    "\n",
    "## Types of transformations\n",
    "\n",
    "### Narrow\n",
    "\n",
    "Act only on one partition of the output data (Map, Filter)\n",
    "\n",
    "- Does not requires to share data within another workers\n",
    "- Are independent of other partitions\n",
    "- Are computationally efficient\n",
    "\n",
    "### Wide\n",
    "\n",
    "Can work on several partitions of output data (Join, GroupBy)\n",
    "\n",
    "## Running locally\n",
    "\n",
    "1. Docker Compose file:\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "  glue:\n",
    "    image: amazon/aws-glue-libs:glue_libs_4.0.0_image_01\n",
    "    container_name: glue-jupyter\n",
    "    ports:\n",
    "      - \"8889:8888\"\n",
    "      - \"4040:4040\"\n",
    "    volumes:\n",
    "      - .:/home/glue_user/workspace/jupyter_workspace\n",
    "      - ~/.aws:/home/glue_user/.aws:ro\n",
    "    working_dir: /home/glue_user/workspace\n",
    "    environment:\n",
    "      AWS_PROFILE: default\n",
    "      DISABLE_SSL: true\n",
    "    command: >\n",
    "      /home/glue_user/jupyter/jupyter_start.sh\n",
    "```\n",
    "\n",
    "2. Run image:\n",
    "\n",
    "```sh\n",
    "docker compose run --rm glue\n",
    "```\n",
    "\n",
    "3. Connect to jupyter server within vscode:\n",
    "\n",
    "- Select kernel\n",
    "- Existing jupyter server\n",
    "- https://127.0.1:8889/lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b8cff",
   "metadata": {},
   "source": [
    "Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed2dd6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aad9eb9912e4b859f474b44df8b8f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc54e1",
   "metadata": {},
   "source": [
    "Create session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd5ec8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769cc1bb2bf84d908f7e0c051a92248b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c2c83",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "Create a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cee167",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffcbc19e5c44b07abb125d2788f2cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    (\"John\", \"Sales\", 3000),\n",
    "    (\"Bryan\", \"Sales\", 4200),\n",
    "    (\"Selena\", \"Sales\", 4600),\n",
    "    (\"Alice\", \"Sales\", 3500),\n",
    "    (\"Mark\", \"Sales\", 3900),\n",
    "    (\"Sophia\", \"Sales\", 4100),\n",
    "    (\"Daniel\", \"Sales\", 3800),\n",
    "    (\"Emma\", \"Sales\", 4400),\n",
    "    (\"Lucas\", \"Sales\", 3600),\n",
    "    (\"Olivia\", \"Sales\", 4700),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"name\", \"department\", \"salary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d9f438",
   "metadata": {},
   "source": [
    "Visualize data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d55b6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6787d66c18468194fd0472a30a6937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n",
      "|  name|department|salary|\n",
      "+------+----------+------+\n",
      "|  John|     Sales|  3000|\n",
      "| Bryan|     Sales|  4200|\n",
      "|Selena|     Sales|  4600|\n",
      "| Alice|     Sales|  3500|\n",
      "|  Mark|     Sales|  3900|\n",
      "|Sophia|     Sales|  4100|\n",
      "|Daniel|     Sales|  3800|\n",
      "|  Emma|     Sales|  4400|\n",
      "| Lucas|     Sales|  3600|\n",
      "|Olivia|     Sales|  4700|\n",
      "+------+----------+------+"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893aaaf9",
   "metadata": {},
   "source": [
    "Filtering dataset (narrow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6aa789",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b322512e00ae42c4a486d15db8621d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n",
      "|  name|department|salary|\n",
      "+------+----------+------+\n",
      "| Bryan|     Sales|  4200|\n",
      "|Selena|     Sales|  4600|\n",
      "|Sophia|     Sales|  4100|\n",
      "|  Emma|     Sales|  4400|\n",
      "|Olivia|     Sales|  4700|\n",
      "+------+----------+------+"
     ]
    }
   ],
   "source": [
    "df_filtered = df.filter(df[\"salary\"] > 4000)\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263eb20d",
   "metadata": {},
   "source": [
    "Group By (Wide)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def64b74",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdc231df3704cee80872fabaf6c37ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|   10|\n",
      "+----------+-----+"
     ]
    }
   ],
   "source": [
    "df_grouped = df.groupby(\"department\").count()\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9b0cb",
   "metadata": {},
   "source": [
    "## Pushdown Predicate\n",
    "\n",
    "Reduce the total data read applying conditional filters to the data columns.\n",
    "\n",
    "- Depends on file formats (Parquet and ORC)\n",
    "\n",
    "1. Query submitted to spark\n",
    "2. Planning phase using Catalyst optimizer\n",
    "3. Filters identification (predicate)\n",
    "4. Pushdown: try to bring the filter to reading\n",
    "\n",
    "Using pure spark `WHERE` clause is enough, when using _AWS Glue_, `pushdown_predicate` argument is needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af69f2",
   "metadata": {},
   "source": [
    "## Catalyst\n",
    "\n",
    "Is a mechanism to optimize spark jobs utilizing functional programming principles to identify logical plan improvements (performance and execution time)\n",
    "\n",
    "Catalyst has 19 rules (Spark 3+) to improve query execution.\n",
    "\n",
    "Example:\n",
    "\n",
    "- Pushdown predicate\n",
    "- Rearrange filters\n",
    "- Decimal operations conversion to long integer\n",
    "- Regex rewriting in Java native code (startswith and contains)\n",
    "- If-else simplification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21d308ac",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0378f3b1aa74ed9b46f7104c6f07c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----------+-----------+------------+\n",
      "|employee_id|  name|department|base_salary|annual_sales|\n",
      "+-----------+------+----------+-----------+------------+\n",
      "|          1|  John|     Sales|       3000|      120000|\n",
      "|          2| Bryan|     Sales|       4200|      180000|\n",
      "|          3|Selena|     Sales|       4600|      220000|\n",
      "|          4| Alice|     Sales|       3500|       95000|\n",
      "|          5|  Mark|     Sales|       3900|      160000|\n",
      "+-----------+------+----------+-----------+------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (11)\n",
      "+- Project (10)\n",
      "   +- SortMergeJoin Inner (9)\n",
      "      :- Sort (4)\n",
      "      :  +- Exchange (3)\n",
      "      :     +- Filter (2)\n",
      "      :        +- Scan ExistingRDD (1)\n",
      "      +- Sort (8)\n",
      "         +- Exchange (7)\n",
      "            +- Filter (6)\n",
      "               +- Scan ExistingRDD (5)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD\n",
      "Output [4]: [employee_id#186L, name#187, department#188, base_salary#189L]\n",
      "Arguments: [employee_id#186L, name#187, department#188, base_salary#189L], MapPartitionsRDD[81] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Filter\n",
      "Input [4]: [employee_id#186L, name#187, department#188, base_salary#189L]\n",
      "Condition : isnotnull(employee_id#186L)\n",
      "\n",
      "(3) Exchange\n",
      "Input [4]: [employee_id#186L, name#187, department#188, base_salary#189L]\n",
      "Arguments: hashpartitioning(employee_id#186L, 1000), ENSURE_REQUIREMENTS, [id=#862]\n",
      "\n",
      "(4) Sort\n",
      "Input [4]: [employee_id#186L, name#187, department#188, base_salary#189L]\n",
      "Arguments: [employee_id#186L ASC NULLS FIRST], false, 0\n",
      "\n",
      "(5) Scan ExistingRDD\n",
      "Output [2]: [employee_id#194L, annual_sales#195L]\n",
      "Arguments: [employee_id#194L, annual_sales#195L], MapPartitionsRDD[86] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(6) Filter\n",
      "Input [2]: [employee_id#194L, annual_sales#195L]\n",
      "Condition : isnotnull(employee_id#194L)\n",
      "\n",
      "(7) Exchange\n",
      "Input [2]: [employee_id#194L, annual_sales#195L]\n",
      "Arguments: hashpartitioning(employee_id#194L, 1000), ENSURE_REQUIREMENTS, [id=#863]\n",
      "\n",
      "(8) Sort\n",
      "Input [2]: [employee_id#194L, annual_sales#195L]\n",
      "Arguments: [employee_id#194L ASC NULLS FIRST], false, 0\n",
      "\n",
      "(9) SortMergeJoin\n",
      "Left keys [1]: [employee_id#186L]\n",
      "Right keys [1]: [employee_id#194L]\n",
      "Join condition: None\n",
      "\n",
      "(10) Project\n",
      "Output [5]: [employee_id#186L, name#187, department#188, base_salary#189L, annual_sales#195L]\n",
      "Input [6]: [employee_id#186L, name#187, department#188, base_salary#189L, employee_id#194L, annual_sales#195L]\n",
      "\n",
      "(11) AdaptiveSparkPlan\n",
      "Output [5]: [employee_id#186L, name#187, department#188, base_salary#189L, annual_sales#195L]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"catalyst-join\").getOrCreate()\n",
    "\n",
    "employees_data = [\n",
    "    (1, \"John\", \"Sales\", 3000),\n",
    "    (2, \"Bryan\", \"Sales\", 4200),\n",
    "    (3, \"Selena\", \"Sales\", 4600),\n",
    "    (4, \"Alice\", \"Sales\", 3500),\n",
    "    (5, \"Mark\", \"Sales\", 3900),\n",
    "]\n",
    "\n",
    "df_employees = spark.createDataFrame(\n",
    "    employees_data, [\"employee_id\", \"name\", \"department\", \"base_salary\"]\n",
    ")\n",
    "\n",
    "sales_data = [\n",
    "    (1, 120_000),\n",
    "    (2, 180_000),\n",
    "    (3, 220_000),\n",
    "    (4, 95_000),\n",
    "    (5, 160_000),\n",
    "]\n",
    "\n",
    "df_sales = spark.createDataFrame(sales_data, [\"employee_id\", \"annual_sales\"])\n",
    "\n",
    "spark.sparkContext.setJobGroup(\n",
    "    \"JOIN\", \"Join employees with annual sales performance\"\n",
    ")\n",
    "\n",
    "df_join_result = df_employees.join(df_sales, on=\"employee_id\", how=\"inner\")\n",
    "\n",
    "df_join_result.show()\n",
    "\n",
    "df_join_result.explain(mode=\"formatted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
