{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207e423a",
   "metadata": {},
   "source": [
    "# Glue Spark\n",
    "\n",
    "- RDD: Datasets which storage data in distributed format\n",
    "- Transformations: Datasets iterations\n",
    "- Actions: Run the transformations on RDD\n",
    "- Output data: Result of transformations\n",
    "\n",
    "Spark runs lazy, so transformations are not immediately executed, they stay in a logical plan to be executed when the actions are called.\n",
    "\n",
    "Actions examples:\n",
    "\n",
    "- `take()`\n",
    "- `collect()`\n",
    "- `show()`\n",
    "- `save()`\n",
    "- `count()`\n",
    "\n",
    "## Types of transformations\n",
    "\n",
    "### Narrow\n",
    "\n",
    "Act only on one partition of the output data (Map, Filter)\n",
    "\n",
    "- Does not requires to share data within another workers\n",
    "- Are independent of other partitions\n",
    "- Are computationally efficient\n",
    "\n",
    "### Wide\n",
    "\n",
    "Can work on several partitions of output data (Join, GroupBy)\n",
    "\n",
    "## Running locally\n",
    "\n",
    "1. Docker Compose file:\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "  glue:\n",
    "    image: amazon/aws-glue-libs:glue_libs_4.0.0_image_01\n",
    "    container_name: glue-jupyter\n",
    "    ports:\n",
    "      - \"8889:8888\"\n",
    "      - \"4040:4040\"\n",
    "    volumes:\n",
    "      - .:/home/glue_user/workspace/jupyter_workspace\n",
    "      - ~/.aws:/home/glue_user/.aws:ro\n",
    "    working_dir: /home/glue_user/workspace\n",
    "    environment:\n",
    "      AWS_PROFILE: default\n",
    "      DISABLE_SSL: true\n",
    "    command: >\n",
    "      /home/glue_user/jupyter/jupyter_start.sh\n",
    "```\n",
    "\n",
    "2. Run image:\n",
    "\n",
    "```sh\n",
    "docker compose run --rm glue\n",
    "```\n",
    "\n",
    "3. Connect to jupyter server within vscode:\n",
    "\n",
    "- Select kernel\n",
    "- Existing jupyter server\n",
    "- https://127.0.1:8889/lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b8cff",
   "metadata": {},
   "source": [
    "Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed2dd6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e0453c53e144369b600a96871d0642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from awsglue.context import GlueContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc54e1",
   "metadata": {},
   "source": [
    "Create session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd5ec8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f1f328c3224cca98998d1a772d787e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c2c83",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "Create a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cee167",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64f05ab87414622b8a7a3a99cf918e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    (\"John\", \"Sales\", 3000),\n",
    "    (\"Bryan\", \"Sales\", 4200),\n",
    "    (\"Selena\", \"Sales\", 4600),\n",
    "    (\"Alice\", \"Sales\", 3500),\n",
    "    (\"Mark\", \"Sales\", 3900),\n",
    "    (\"Sophia\", \"Sales\", 4100),\n",
    "    (\"Daniel\", \"Sales\", 3800),\n",
    "    (\"Emma\", \"Sales\", 4400),\n",
    "    (\"Lucas\", \"Sales\", 3600),\n",
    "    (\"Olivia\", \"Sales\", 4700),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"name\", \"department\", \"salary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d9f438",
   "metadata": {},
   "source": [
    "Visualize data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d55b6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab29925dc1d4eac86ac5e917bad5d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n",
      "|  name|department|salary|\n",
      "+------+----------+------+\n",
      "|  John|     Sales|  3000|\n",
      "| Bryan|     Sales|  4200|\n",
      "|Selena|     Sales|  4600|\n",
      "| Alice|     Sales|  3500|\n",
      "|  Mark|     Sales|  3900|\n",
      "|Sophia|     Sales|  4100|\n",
      "|Daniel|     Sales|  3800|\n",
      "|  Emma|     Sales|  4400|\n",
      "| Lucas|     Sales|  3600|\n",
      "|Olivia|     Sales|  4700|\n",
      "+------+----------+------+"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893aaaf9",
   "metadata": {},
   "source": [
    "Filtering dataset (narrow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6aa789",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c527be1242462694af5752e327cbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n",
      "|  name|department|salary|\n",
      "+------+----------+------+\n",
      "| Bryan|     Sales|  4200|\n",
      "|Selena|     Sales|  4600|\n",
      "|Sophia|     Sales|  4100|\n",
      "|  Emma|     Sales|  4400|\n",
      "|Olivia|     Sales|  4700|\n",
      "+------+----------+------+"
     ]
    }
   ],
   "source": [
    "df_filtered = df.filter(df[\"salary\"] > 4000)\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263eb20d",
   "metadata": {},
   "source": [
    "Group By (Wide)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def64b74",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90efa27d59a4bb99f4e57206826f7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|   10|\n",
      "+----------+-----+"
     ]
    }
   ],
   "source": [
    "df_grouped = df.groupby(\"department\").count()\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9b0cb",
   "metadata": {},
   "source": [
    "## Pushdown Predicate\n",
    "\n",
    "Reduce the total data read applying conditional filters to the data columns.\n",
    "\n",
    "- Depends on file formats (Parquet and ORC)\n",
    "\n",
    "1. Query submitted to spark\n",
    "2. Planning phase using Catalyst optimizer\n",
    "3. Filters identification (predicate)\n",
    "4. Pushdown: try to bring the filter to reading\n",
    "\n",
    "Using pure spark `WHERE` clause is enough, when using _AWS Glue_, `pushdown_predicate` argument is needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af69f2",
   "metadata": {},
   "source": [
    "## Catalyst\n",
    "\n",
    "Is a mechanism to optimize spark jobs utilizing functional programming principles to identify logical plan improvements (performance and execution time)\n",
    "\n",
    "Catalyst has 19 rules (Spark 3+) to improve query execution.\n",
    "\n",
    "Example:\n",
    "\n",
    "- Pushdown predicate\n",
    "- Rearrange filters\n",
    "- Decimal operations conversion to long integer\n",
    "- Regex rewriting in Java native code (startswith and contains)\n",
    "- If-else simplification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d308ac",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2026f2c3045f4076b9724ab531485326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----------+-----------+------------+\n",
      "|employee_id|  name|department|base_salary|annual_sales|\n",
      "+-----------+------+----------+-----------+------------+\n",
      "|          1|  John|     Sales|       3000|      120000|\n",
      "|          2| Bryan|     Sales|       4200|      180000|\n",
      "|          3|Selena|     Sales|       4600|      220000|\n",
      "|          4| Alice|     Sales|       3500|       95000|\n",
      "|          5|  Mark|     Sales|       3900|      160000|\n",
      "+-----------+------+----------+-----------+------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (11)\n",
      "+- Project (10)\n",
      "   +- SortMergeJoin Inner (9)\n",
      "      :- Sort (4)\n",
      "      :  +- Exchange (3)\n",
      "      :     +- Filter (2)\n",
      "      :        +- Scan ExistingRDD (1)\n",
      "      +- Sort (8)\n",
      "         +- Exchange (7)\n",
      "            +- Filter (6)\n",
      "               +- Scan ExistingRDD (5)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD\n",
      "Output [4]: [employee_id#50L, name#51, department#52, base_salary#53L]\n",
      "Arguments: [employee_id#50L, name#51, department#52, base_salary#53L], MapPartitionsRDD[18] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Filter\n",
      "Input [4]: [employee_id#50L, name#51, department#52, base_salary#53L]\n",
      "Condition : isnotnull(employee_id#50L)\n",
      "\n",
      "(3) Exchange\n",
      "Input [4]: [employee_id#50L, name#51, department#52, base_salary#53L]\n",
      "Arguments: hashpartitioning(employee_id#50L, 1000), ENSURE_REQUIREMENTS, [id=#259]\n",
      "\n",
      "(4) Sort\n",
      "Input [4]: [employee_id#50L, name#51, department#52, base_salary#53L]\n",
      "Arguments: [employee_id#50L ASC NULLS FIRST], false, 0\n",
      "\n",
      "(5) Scan ExistingRDD\n",
      "Output [2]: [employee_id#58L, annual_sales#59L]\n",
      "Arguments: [employee_id#58L, annual_sales#59L], MapPartitionsRDD[23] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(6) Filter\n",
      "Input [2]: [employee_id#58L, annual_sales#59L]\n",
      "Condition : isnotnull(employee_id#58L)\n",
      "\n",
      "(7) Exchange\n",
      "Input [2]: [employee_id#58L, annual_sales#59L]\n",
      "Arguments: hashpartitioning(employee_id#58L, 1000), ENSURE_REQUIREMENTS, [id=#260]\n",
      "\n",
      "(8) Sort\n",
      "Input [2]: [employee_id#58L, annual_sales#59L]\n",
      "Arguments: [employee_id#58L ASC NULLS FIRST], false, 0\n",
      "\n",
      "(9) SortMergeJoin\n",
      "Left keys [1]: [employee_id#50L]\n",
      "Right keys [1]: [employee_id#58L]\n",
      "Join condition: None\n",
      "\n",
      "(10) Project\n",
      "Output [5]: [employee_id#50L, name#51, department#52, base_salary#53L, annual_sales#59L]\n",
      "Input [6]: [employee_id#50L, name#51, department#52, base_salary#53L, employee_id#58L, annual_sales#59L]\n",
      "\n",
      "(11) AdaptiveSparkPlan\n",
      "Output [5]: [employee_id#50L, name#51, department#52, base_salary#53L, annual_sales#59L]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"catalyst-join\").getOrCreate()\n",
    "\n",
    "employees_data = [\n",
    "    (1, \"John\", \"Sales\", 3000),\n",
    "    (2, \"Bryan\", \"Sales\", 4200),\n",
    "    (3, \"Selena\", \"Sales\", 4600),\n",
    "    (4, \"Alice\", \"Sales\", 3500),\n",
    "    (5, \"Mark\", \"Sales\", 3900),\n",
    "]\n",
    "\n",
    "df_employees = spark.createDataFrame(\n",
    "    employees_data, [\"employee_id\", \"name\", \"department\", \"base_salary\"]\n",
    ")\n",
    "\n",
    "sales_data = [\n",
    "    (1, 120_000),\n",
    "    (2, 180_000),\n",
    "    (3, 220_000),\n",
    "    (4, 95_000),\n",
    "    (5, 160_000),\n",
    "]\n",
    "\n",
    "df_sales = spark.createDataFrame(sales_data, [\"employee_id\", \"annual_sales\"])\n",
    "\n",
    "spark.sparkContext.setJobGroup(\n",
    "    \"JOIN\", \"Join employees with annual sales performance\"\n",
    ")\n",
    "\n",
    "df_join_result = df_employees.join(df_sales, on=\"employee_id\", how=\"inner\")\n",
    "\n",
    "df_join_result.show()\n",
    "\n",
    "df_join_result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac3f58",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "- [different-types-of-join-strategies-in-apache-spark](https://oindrila-chakraborty88.medium.com/different-types-of-join-strategies-in-apache-spark-5c0066999d0d)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "A[Start] --> B{Has user hint?};\n",
    "B --> |Yes| C[Apply desired user join]\n",
    "B --> |No| D{Dataset size allows to apply broadcast?}\n",
    "D --> |Yes| E[Broadcast Join]\n",
    "D --> |No| F{It fits on worker memory?}\n",
    "F --> |Yes| G[Shuffle Hash Join]\n",
    "F --> |No| H[Sort Merge Join - Default]\n",
    "```\n",
    "\n",
    "### Broadcast Join\n",
    "\n",
    "When `broadcast` join type is used, it avoids shuffle that is an expensive network exchange between nodes. This improve time, performance and cost.\n",
    "\n",
    "When a dataset small, the duplication of the dataset in in all nodes is more effective to do joins. Spark defaults to a limit of _10MB_ to do it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee9f54c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92e78fce37244e0938f7c61a2a92110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+------------+\n",
      "|country_id|customer_id|customer_name|country_name|\n",
      "+----------+-----------+-------------+------------+\n",
      "|         1|        101|        Alice|      Brazil|\n",
      "|         2|        102|          Bob|         USA|\n",
      "|         1|        103|        Carol|      Brazil|\n",
      "|         3|        104|         Dave|     Germany|\n",
      "+----------+-----------+-------------+------------+"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"broadcast-join-example\").getOrCreate()\n",
    "\n",
    "countries_data = [\n",
    "    (1, \"Brazil\"),\n",
    "    (2, \"USA\"),\n",
    "    (3, \"Germany\"),\n",
    "]\n",
    "\n",
    "df_countries = spark.createDataFrame(\n",
    "    countries_data, [\"country_id\", \"country_name\"]\n",
    ")\n",
    "\n",
    "customers_data = [\n",
    "    (101, \"Alice\", 1),\n",
    "    (102, \"Bob\", 2),\n",
    "    (103, \"Carol\", 1),\n",
    "    (104, \"Dave\", 3),\n",
    "]\n",
    "\n",
    "df_customers = spark.createDataFrame(\n",
    "    customers_data, [\"customer_id\", \"customer_name\", \"country_id\"]\n",
    ")\n",
    "\n",
    "spark.sparkContext.setJobGroup(\n",
    "    \"BROADCAST_JOIN\", \"Join customers with country lookup using broadcast\"\n",
    ")\n",
    "\n",
    "df_result = df_customers.join(\n",
    "    broadcast(df_countries), on=\"country_id\", how=\"inner\"\n",
    ")\n",
    "\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c9960d",
   "metadata": {},
   "source": [
    "Increasing the limit for broadcast join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9253af",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13dfc41db32341de9017d3c7a7058b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"boardcast-join-threshold\")\n",
    "    .config(\n",
    "        \"spark.sql.autoBroadcastJoinThreshold\",\n",
    "        str(3 * 1024 * 1024 * 1024),  # 3GB\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "glue_context = GlueContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5664067",
   "metadata": {},
   "source": [
    "After `BroadcastJoin`, `ShuffleHashJoin` is more performant because it does not sort values like `SortMergeJoin` that is the _Spark default_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a9bdb",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b520732ac29b45a59a317aa29fd23769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"hash-shuffle-join\")\n",
    "    .config(\"spark.sql.join.preferSortMergeJoin\", \"false\")\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "glue_context = GlueContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1396d7e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "When we have a large volume of data and neither of previous joins work `SortMergeJoin` will be the most flexible. The join occurs through sorted keys in each partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0d680",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d593f357b04f9fa79b6a3c8702bed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"sort-merge-join\")\n",
    "    .config(\"spark.sql.join.preferSortMergeJoin\", \"true\")\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "glue_context = GlueContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910256d",
   "metadata": {},
   "source": [
    "## Repartition\n",
    "\n",
    "Partition manipulation method, can be triggered manually or by `AdaptativeQueryExecution`\n",
    "\n",
    "- Do not do shuffle\n",
    "- Create partitions with different sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f37e74",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "## Coalesce\n",
    "\n",
    "Partition manipulation method, can be triggered manually or by `AdaptativeQueryExecution`\n",
    "\n",
    "- Increase or decrease the number of partitions\n",
    "- Create partitions with equal sizes using shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd706b5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "## Shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb3d4a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "## Spills\n",
    "\n",
    "When data does not fit on memory or storage. Some times can lead to `Out Of Memory Error`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2a1c15",
   "metadata": {},
   "source": [
    "## Persist\n",
    "\n",
    "Stores the already done computations in partitions avoiding redoing shuffles or other operations. It should be applied when the same dataset will be used multiple times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4719e749",
   "metadata": {},
   "source": [
    "## Configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6400a02",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fa6c385bff4076a00c801a34e592cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.master = local\n",
      "spark.eventLog.enabled = true\n",
      "spark.network.crypto.keyLength = 256\n",
      "spark.network.crypto.enabled = true\n",
      "spark.repl.local.jars = file:///home/glue_user/livy/rsc-jars/arpack_combined_all-0.1.jar,file:///home/glue_user/livy/rsc-jars/core-1.1.2.jar,file:///home/glue_user/livy/rsc-jars/jniloader-1.1.jar,file:///home/glue_user/livy/rsc-jars/livy-api-0.7.1-incubating.jar,file:///home/glue_user/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar,file:///home/glue_user/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar,file:///home/glue_user/livy/rsc-jars/native_ref-java-1.1.jar,file:///home/glue_user/livy/rsc-jars/native_system-java-1.1.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_ref-linux-armhf-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_ref-linux-i686-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_ref-linux-x86_64-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_ref-osx-x86_64-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_ref-win-i686-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_ref-win-x86_64-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_system-linux-armhf-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_system-linux-i686-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_system-linux-x86_64-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_system-osx-x86_64-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_system-win-i686-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_system-win-x86_64-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netty-all-4.1.17.Final.jar,file:///home/glue_user/livy/repl_2.12-jars/commons-codec-1.9.jar,file:///home/glue_user/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar,file:///home/glue_user/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating-LIVY-795-patched.jar\n",
      "spark.app.name = livy-session-1\n",
      "spark.jars = file:///home/glue_user/livy/rsc-jars/arpack_combined_all-0.1.jar,file:///home/glue_user/livy/rsc-jars/core-1.1.2.jar,file:///home/glue_user/livy/rsc-jars/jniloader-1.1.jar,file:///home/glue_user/livy/rsc-jars/livy-api-0.7.1-incubating.jar,file:///home/glue_user/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar,file:///home/glue_user/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar,file:///home/glue_user/livy/rsc-jars/native_ref-java-1.1.jar,file:///home/glue_user/livy/rsc-jars/native_system-java-1.1.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_ref-linux-armhf-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_ref-linux-i686-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_ref-linux-x86_64-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_ref-osx-x86_64-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_ref-win-i686-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_ref-win-x86_64-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_system-linux-armhf-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_system-linux-i686-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_system-linux-x86_64-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_system-osx-x86_64-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_system-win-i686-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netlib-native_system-win-x86_64-1.1-natives.jar,file:///home/glue_user/livy/rsc-jars/netty-all-4.1.17.Final.jar,file:///home/glue_user/livy/repl_2.12-jars/commons-codec-1.9.jar,file:///home/glue_user/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar,file:///home/glue_user/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating-LIVY-795-patched.jar\n",
      "spark.network.crypto.keyFactoryAlgorithm = PBKDF2WithHmacSHA256\n",
      "spark.hadoop.dynamodb.customAWSCredentialsProvider = com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n",
      "spark.yarn.submit.waitAppCompletion = false\n",
      "spark.driver.host = 1db0a48ffb16\n",
      "spark.driver.port = 38725\n",
      "spark.app.startTime = 1766755739882\n",
      "spark.app.initial.jar.urls = spark://1db0a48ffb16:38725/jars/livy-repl_2.12-0.7.1-incubating-LIVY-795-patched.jar,spark://1db0a48ffb16:38725/jars/commons-codec-1.9.jar,spark://1db0a48ffb16:38725/jars/netlib-native_ref-win-i686-1.1-natives.jar,spark://1db0a48ffb16:38725/jars/netlib-native_system-win-i686-1.1-natives.jar,spark://1db0a48ffb16:38725/jars/core-1.1.2.jar,spark://1db0a48ffb16:38725/jars/jniloader-1.1.jar,spark://1db0a48ffb16:38725/jars/netlib-native_system-linux-x86_64-1.1-natives.jar,spark://1db0a48ffb16:38725/jars/native_system-java-1.1.jar,spark://1db0a48ffb16:38725/jars/livy-thriftserver-session-0.7.1-incubating.jar,spark://1db0a48ffb16:38725/jars/netty-all-4.1.17.Final.jar,spark://1db0a48ffb16:38725/jars/livy-api-0.7.1-incubating.jar,spark://1db0a48ffb16:38725/jars/netlib-native_ref-linux-i686-1.1-natives.jar,spark://1db0a48ffb16:38725/jars/netlib-native_system-win-x86_64-1.1-natives.jar,spark://1db0a48ffb16:38725/jars/netlib-native_ref-linux-x86_64-1.1-natives.jar,spark://1db0a48ffb16:38725/jars/netlib-native_system-linux-armhf-1.1-natives.jar,spark://1db0a48ffb16:38725/jars/netlib-native_ref-linux-armhf-1.1-natives.jar,spark://1db0a48ffb16:38725/jars/netlib-native_system-osx-x86_64-1.1-natives.jar,spark://1db0a48ffb16:38725/jars/netlib-native_ref-osx-x86_64-1.1-natives.jar,spark://1db0a48ffb16:38725/jars/livy-core_2.12-0.7.1-incubating.jar,spark://1db0a48ffb16:38725/jars/arpack_combined_all-0.1.jar,spark://1db0a48ffb16:38725/jars/livy-rsc-0.7.1-incubating.jar,spark://1db0a48ffb16:38725/jars/native_ref-java-1.1.jar,spark://1db0a48ffb16:38725/jars/netlib-native_system-linux-i686-1.1-natives.jar,spark://1db0a48ffb16:38725/jars/netlib-native_ref-win-x86_64-1.1-natives.jar\n",
      "spark.io.encryption.enabled = false\n",
      "spark.yarn.maxAppAttempts = 1\n",
      "spark.yarn.dist.archives = \n",
      "spark.sql.dataPrefetch.enabled = false\n",
      "spark.files = file:///home/glue_user/spark/conf/hive-site.xml\n",
      "spark.yarn.heterogeneousExecutors.enabled = false\n",
      "spark.submit.deployMode = client\n",
      "spark.authenticate.secret = fca10096-9993-458c-b331-d05073d788b5\n",
      "spark.repl.class.uri = spark://1db0a48ffb16:38725/classes\n",
      "spark.app.id = local-1766755741527\n",
      "spark.livy.spark_major_version = 3\n",
      "spark.app.submitTime = 1766755730934\n",
      "spark.executor.extraClassPath = /home/glue_user/spark/jars/*:/home/glue_user/aws-glue-libs/jars/*\n",
      "spark.executor.id = driver\n",
      "spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs = false\n",
      "spark.unsafe.sorter.spill.read.ahead.enabled = false\n",
      "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version = 2\n",
      "spark.executor.cores = 2\n",
      "spark.sql.warehouse.dir = file:/home/glue_user/workspace/spark-warehouse\n",
      "spark.repl.class.outputDir = /tmp/spark668078796105942909\n",
      "spark.driver.extraJavaOptions = -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djavax.net.ssl.trustStore=/home/glue_user/.certs/aws_tls_certs/InternalAndExternalAndAWSTrustStore.jks -Djavax.net.ssl.trustStoreType=JKS -Djavax.net.ssl.trustStorePassword=amazon\n",
      "spark.history.fs.logDirectory = file:////tmp/spark-events\n",
      "spark.sql.catalogImplementation = hive\n",
      "spark.executor.extraJavaOptions = -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djavax.net.ssl.trustStore=/home/glue_user/.certs/aws_tls_certs/InternalAndExternalAndAWSTrustStore.jks -Djavax.net.ssl.trustStoreType=JKS -Djavax.net.ssl.trustStorePassword=amazon\n",
      "spark.app.initial.file.urls = file:///home/glue_user/spark/conf/hive-site.xml\n",
      "spark.network.crypto.saslFallback = false\n",
      "spark.submit.pyFiles = \n",
      "spark.yarn.isPython = true\n",
      "spark.driver.extraClassPath = /home/glue_user/spark/jars/*:/home/glue_user/aws-glue-libs/jars/*\n",
      "spark.authenticate = true\n",
      "spark.driver.memory = 1000M"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"optimized-spark-session\")\n",
    "\n",
    "\n",
    "config_params = {}\n",
    "\n",
    "for key, value in config_params.items():\n",
    "    spark = spark.config(key, value)\n",
    "\n",
    "spark = spark.enableHiveSupport().getOrCreate()\n",
    "\n",
    "all_configs = spark.sparkContext.getConf().getAll()\n",
    "\n",
    "for config in all_configs:\n",
    "    print(f\"{config[0]} = {config[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
