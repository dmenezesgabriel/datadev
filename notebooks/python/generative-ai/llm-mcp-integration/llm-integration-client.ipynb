{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e1e56ce",
   "metadata": {},
   "source": [
    "# MCP - LLM Integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f436236",
   "metadata": {},
   "source": [
    "Create MCP server file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e5e6c",
   "metadata": {},
   "source": [
    "```py title='server.py'\n",
    "--8<-- \"docs/notebooks/python/generative-ai/llm-mcp-integration/server.py\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ede863f",
   "metadata": {},
   "source": [
    "Install packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5acd5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q \\\n",
    "    litellm==1.78.5 \\\n",
    "    python-dotenv==1.1.1 \\\n",
    "    pydantic==2.12.3 \\\n",
    "    mcp==1.21.0 \\\n",
    "    nest-asyncio==1.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b5e15",
   "metadata": {},
   "source": [
    "Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e5ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from contextlib import AsyncExitStack\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import litellm  # type: ignore\n",
    "import nest_asyncio  # type: ignore\n",
    "from dotenv import load_dotenv  # type: ignore\n",
    "from mcp import ClientSession, StdioServerParameters  # type: ignore\n",
    "from mcp.client.stdio import stdio_client  # type: ignore\n",
    "\n",
    "nest_asyncio.apply()  # Needed to run interactive python (Jupyter Notebooks)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf5041",
   "metadata": {},
   "source": [
    "Define client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ca1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCPLiteLLMClient:\n",
    "    def __init__(\n",
    "        self, model: str = \"gemini/gemini-2.0-flash\", max_iterations: int = 3\n",
    "    ):\n",
    "        self.session: Optional[ClientSession] = None\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.model = model\n",
    "        self.messages: Optional[List[Dict[str, Any]]] = None\n",
    "        self.stdio: Optional[Any] = None\n",
    "        self.write: Optional[Any] = None\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    async def connect(self, server_script_path: str = \"server.py\"):\n",
    "        server_params = StdioServerParameters(\n",
    "            command=\"uv\", args=[\"run\", server_script_path]\n",
    "        )\n",
    "\n",
    "        stdio_transport = await self.exit_stack.enter_async_context(\n",
    "            stdio_client(server_params)\n",
    "        )\n",
    "        self.stdio, self.write = stdio_transport\n",
    "        self.session = await self.exit_stack.enter_async_context(\n",
    "            ClientSession(self.stdio, self.write)\n",
    "        )\n",
    "\n",
    "        if self.session is None:\n",
    "            raise Exception(\"No session\")\n",
    "\n",
    "        await self.session.initialize()\n",
    "\n",
    "        available_tools = await self.session.list_tools()\n",
    "        print(\"Available tools:\")\n",
    "        for tool in available_tools.tools:\n",
    "            print(f\"  - {tool.name}: {tool.description}\")\n",
    "\n",
    "    async def disconnect(self):\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "    async def get_available_tools(self) -> List[Dict[str, Any]]:\n",
    "        if self.session is None:\n",
    "            raise Exception(\"No session\")\n",
    "\n",
    "        tools_result = await self.session.list_tools()\n",
    "        return [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"parameters\": tool.inputSchema,\n",
    "                },\n",
    "            }\n",
    "            for tool in tools_result.tools\n",
    "        ]\n",
    "\n",
    "    async def process_query(self, query: str):\n",
    "        if self.session is None:\n",
    "            raise Exception(\"No session\")\n",
    "\n",
    "        tools = await self.get_available_tools()\n",
    "\n",
    "        if not self.messages:\n",
    "            self.messages = [{\"role\": \"user\", \"content\": query}]\n",
    "        else:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "        current_iteration = 0\n",
    "\n",
    "        while current_iteration <= self.max_iterations:\n",
    "            current_iteration += 1\n",
    "\n",
    "            response = litellm.completion(\n",
    "                model=self.model,\n",
    "                messages=self.messages,\n",
    "                tools=tools,\n",
    "            )\n",
    "\n",
    "            choice = response.choices[0].message\n",
    "            tool_calls = getattr(choice, \"tool_calls\", None)\n",
    "\n",
    "            if not tool_calls:\n",
    "                self.messages.append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": choice.content,\n",
    "                    }\n",
    "                )\n",
    "                return choice.content\n",
    "\n",
    "            self.messages.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": choice.content,\n",
    "                    \"tool_calls\": [\n",
    "                        {\n",
    "                            \"id\": tc.id,\n",
    "                            \"type\": \"function\",\n",
    "                            \"function\": {\n",
    "                                \"name\": tc.function.name,\n",
    "                                \"arguments\": tc.function.arguments,\n",
    "                            },\n",
    "                        }\n",
    "                        for tc in tool_calls\n",
    "                    ],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            for tool_call in tool_calls:\n",
    "                name = tool_call.function.name\n",
    "                kwargs = json.loads(tool_call.function.arguments)\n",
    "                result = await self.session.call_tool(name, arguments=kwargs)\n",
    "\n",
    "                if hasattr(result, \"content\"):\n",
    "                    content_str = json.dumps(\n",
    "                        [\n",
    "                            (\n",
    "                                {\"type\": item.type, \"text\": item.text}\n",
    "                                if hasattr(item, \"text\")\n",
    "                                else str(item)\n",
    "                            )\n",
    "                            for item in result.content\n",
    "                        ]\n",
    "                    )\n",
    "                else:\n",
    "                    content_str = str(result)\n",
    "\n",
    "                self.messages.append(\n",
    "                    {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"content\": content_str,\n",
    "                    }\n",
    "                )\n",
    "        return \"Reach max iterations\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886bbe65",
   "metadata": {},
   "source": [
    "Run client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a73e239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "  - list_tables: List all tables in the database\n",
      "  - create_table: Create a new table. columns should be SQL column definitions like 'id INTEGER, name VARCHAR, amount DECIMAL(10,2)'\n",
      "  - insert_data: Insert data into table. columns like 'name, amount' and values like \"'Product A', 100.50\" \n",
      "  - query_data: Execute SELECT query and return results\n",
      "  - create_table_from_csv: Create a table from a CSV URL. csv_url should be an HTTP(S) URL to a CSV file\n",
      "  - drop_table: Drop a table from the database\n",
      "OK. I have dropped the table named iris.\n",
      "\n",
      "I have created the table iris from the given CSV URL.\n",
      "\n",
      "| sepal_length | sepal_width | petal_length | petal_width | species   |\n",
      "|--------------|-------------|--------------|-------------|-----------|\n",
      "| 5.1          | 3.5         | 1.4          | 0.2         | setosa    |\n",
      "| 4.9          | 3.0         | 1.4          | 0.2         | setosa    |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    client = MCPLiteLLMClient()\n",
    "    await client.connect()\n",
    "\n",
    "    drop_table_query = \"Drop table iris if exists\"\n",
    "    drop_table_response = await client.process_query(drop_table_query)\n",
    "    print(drop_table_response)\n",
    "\n",
    "    create_table_query = \"Create a table named iris from the csv https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv\"\n",
    "    create_table_response = await client.process_query(create_table_query)\n",
    "    print(create_table_response)\n",
    "\n",
    "    select_query = (\n",
    "        \"Show me first 2 items of iris table in a markdown table format\"\n",
    "    )\n",
    "    select_response = await client.process_query(select_query)\n",
    "    print(select_response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv-3.12",
   "language": "python",
   "name": "uv-3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
