{
  "metadata": {
    "kernelspec": {
      "name": "xjavascript",
      "display_name": "JavaScript (xjavascript)",
      "language": "javascript"
    },
    "language_info": {
      "codemirror_mode": "",
      "file_extension": "js",
      "mimetype": "text/x-javascript",
      "name": "javascript",
      "nbconvert_exporter": "",
      "pygments_lexer": "",
      "version": "ES6"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "bf7f42a6-9cc5-4822-a85d-bb5b442cd81c",
      "cell_type": "markdown",
      "source": "# Web LLM\n\n- [docs](https://webllm.mlc.ai/docs/user/basic_usage.html)",
      "metadata": {}
    },
    {
      "id": "119122f5-9602-4a0c-b3a0-5801730be0d7",
      "cell_type": "code",
      "source": "import { CreateMLCEngine, MLCEngine } from \"https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.78/+esm\";",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "id": "9bb4c83e-1413-4261-89d7-7ebfab6fb28b",
      "cell_type": "code",
      "source": " // Initialize with a progress callback\n const initProgressCallback = (progress) => {\n     console.log(progress.text);\n };\n\n// Using CreateMLCEngine\nconst engine = await CreateMLCEngine(\"TinyLlama-1.1B-Chat-v0.4-q4f32_1-MLC-1k\", { initProgressCallback });\n\n// Direct instantiation\nconst engineInstance = new MLCEngine({ initProgressCallback });\nawait engineInstance.reload(\"TinyLlama-1.1B-Chat-v0.4-q4f32_1-MLC-1k\");",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Start to fetch params\nLoading model from cache[1/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[2/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[3/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[4/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[5/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[6/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[7/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[8/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[9/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[10/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[11/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[12/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[13/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[14/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[15/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[16/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[17/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[18/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[19/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[20/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[21/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[22/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[23/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[24/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading GPU shader modules[73/73]: 100% completed, 1 secs elapsed.\nFinish loading on WebGPU - amd\nStart to fetch params\nLoading model from cache[1/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[2/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[3/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[4/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[5/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[6/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[7/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[8/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[9/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[10/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[11/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[12/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[13/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[14/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[15/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[16/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[17/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[18/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[19/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[20/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[21/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[22/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[23/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading model from cache[24/24]: 0MB loaded. 0% completed, 0 secs elapsed.\nLoading GPU shader modules[73/73]: 100% completed, 1 secs elapsed.\nFinish loading on WebGPU - amd\n"
        }
      ],
      "execution_count": 2
    },
    {
      "id": "adfccaca-4703-442e-ad6a-48100cb21cfb",
      "cell_type": "code",
      "source": "const messages = [\n    { role: \"system\", content: \"You are a helpful AI assistant.\" },\n    { role: \"user\", content: \"Hello!\" },\n    { role: \"user\", content: \"write a binary search funcion in **Python** programming language, **just code no explanation**.\" }\n];\n\nconst reply = await engine.chat.completions.create({\n    messages,\n});\n\nconsole.log((reply.choices[0].message.content))\n// ijs.display.json(reply.usage);",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Sure! Here's an implementation in Python:\n```\ndef binary_search(arr, key):\n    low = 0\n    high = len(arr) - 1\n\n    while low < high:\n        mid = (low + high) // 2\n        if key <= arr[mid]:\n            # If the key is less than the current mid, then the search was successful\n            return mid\n        elif key > arr[mid]:\n            # If the key is greater than the current mid, then the search was successful\n            high = mid - 1\n        else:\n            # If the key is equal to the current mid, then the search was unsuccessful\n            return -1\n    # Search was unsuccessful\n    return -1\n```\n"
        },
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {},
          "metadata": {}
        }
      ],
      "execution_count": 7
    }
  ]
}